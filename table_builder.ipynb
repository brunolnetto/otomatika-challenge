{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file json_responses/news_160161.json: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "def search_and_save(\n",
    "    url_template: str, search_term: str, output_dir: str\n",
    "):\n",
    "    # Replace the placeholder with the search term\n",
    "    search_url = url_template.format(search_term=search_term)\n",
    "    logging.info(f\"Searching for '{search_term}' using {search_url}\")\n",
    "    \n",
    "    # Create directory to save the search results pages\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the domain name to use as part of the filename\n",
    "    domain = search_url.split('/')[2]\n",
    "    \n",
    "    # Fetch the search results page\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        \n",
    "        # Raise error for bad responses (4xx or 5xx)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.info(f\"Error fetching search results: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Save the search results page to the output directory\n",
    "    output_filepath = os.path.join(output_dir, f\"{domain}_search_{search_term}.html\")\n",
    "    \n",
    "    with open(output_filepath, 'wb') as outfile:\n",
    "        outfile.write(response.content)\n",
    "\n",
    "    info_msg=f\"Fetched search results for '{search_term}' from {search_url} and saved as {output_filepath}\"\n",
    "    logging.info(info_msg)\n",
    "\n",
    "    return output_filepath\n",
    "\n",
    "def download_json_responses(\n",
    "    input_file: str, pattern: str, output_dir: str\n",
    "):\n",
    "    # Check if the input file exists\n",
    "    if not os.path.isfile(input_file):\n",
    "        print(f\"Input file '{input_file}' not found.\")\n",
    "        return\n",
    "    \n",
    "    output_file_prefix = 'news'\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Read input file and find all URLs matching the pattern\n",
    "    with open(input_file, 'r') as file:\n",
    "        links = re.findall(pattern, file.read())\n",
    "\n",
    "    # Download each link using requests library\n",
    "    for link in links:\n",
    "        # Extract the news number from the link\n",
    "        news_number = re.search(r'[0-9]+$', link).group()\n",
    "\n",
    "        # Construct the output file path\n",
    "        filename = f\"{output_file_prefix}_{news_number}.json\"\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Perform GET request to download JSON data\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Save the JSON response to file\n",
    "            with open(output_file, 'wb') as outfile:\n",
    "                outfile.write(response.content)\n",
    "            \n",
    "            logging.info(f\"Downloaded {link} to {output_file}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to download {link}. Status code: {response.status_code}\")\n",
    "\n",
    "    logging.info(f\"Downloaded JSON responses from links in {input_file} to {output_dir}/{output_file_prefix}_*.json\")\n",
    "\n",
    "\n",
    "def extract_data(source_folder, search_phrases):\n",
    "    # Assuming 'data' contains the JSON data you provided earlier\n",
    "    filenames = listdir(source_folder)\n",
    "    filenames = [f for f in filenames if f.endswith('.json')]\n",
    "\n",
    "    # Initialize lists to store extracted data\n",
    "    ids = []\n",
    "    titles = []\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    picture_filenames = []\n",
    "    search_phrase_counts = []\n",
    "    contains_money = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        file_path=f'{source_folder}/{filename}'\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                # Extract basic fields\n",
    "                id_ = filename.split('.')[0].split('_')[-1]\n",
    "                title = data['title']\n",
    "                date = data['publication_date']\n",
    "                description = data.get('description', '')\n",
    "                picture_filename = ''\n",
    "                for image_data in data.get('lead_asset', []):\n",
    "                    if image_data['type'] == 'lead_image':    \n",
    "                        picture_filename = image_data['value']['image']['file']\n",
    "                        break \n",
    "                \n",
    "                # Count search phrases in title and description\n",
    "                title_search_count = sum(1 for phrase in search_phrases if phrase in title)\n",
    "                description_search_count = sum(1 for phrase in search_phrases if phrase in description)\n",
    "\n",
    "                # Check if title or description contains any money mention\n",
    "                # Regular expression pattern to detect money formats\n",
    "                money_pattern = r'\\$[\\d,]+(\\.\\d+)?|\\d+\\s(dollars|USD)'\n",
    "                \n",
    "                title_has_money = bool(re.search(money_pattern, title))\n",
    "                description_has_money = bool(re.search(money_pattern, description))\n",
    "\n",
    "                # Store extracted data in lists\n",
    "                ids.append(id_)\n",
    "                titles.append(title)\n",
    "                dates.append(date)\n",
    "                descriptions.append(description)\n",
    "                picture_filenames.append(picture_filename)\n",
    "                search_phrase_counts.append(title_search_count + description_search_count)\n",
    "                contains_money.append(title_has_money or description_has_money)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {file_path}: {e}')\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df_data={\n",
    "        'id': ids,\n",
    "        'title': titles,\n",
    "        'date': dates,\n",
    "        'description': descriptions,\n",
    "        'picture_filename': picture_filenames,\n",
    "        'search_phrase_count': search_phrase_counts,\n",
    "        'contains_money': contains_money\n",
    "    }\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "\n",
    "# URL template with placeholders for the search term\n",
    "url_template = \"https://gothamist.com/search?q={search_term}\"\n",
    "search_term='europe'\n",
    "\n",
    "logging.info(f'Searching for {search_term} on {url_template} and saving the search results')\n",
    "filepath = search_and_save(url_template, search_term, 'search_results')\n",
    "\n",
    "if filepath:\n",
    "    # Define the pattern to match URLs\n",
    "    pattern = r'https://api-prod.gothamist.com/api/v2/pages/[0-9]{4,}'\n",
    "\n",
    "    logging.info('Downloading JSON responses')\n",
    "    download_json_responses(filepath, pattern, 'json_responses')\n",
    "\n",
    "shutil.rmtree('search_results')\n",
    "\n",
    "filenames = listdir('json_responses')\n",
    "\n",
    "if not filenames:\n",
    "    logging.info('No JSON responses to process. Exiting...')\n",
    "    exit()\n",
    "else: \n",
    "    # Extract data from JSON responses\n",
    "    logging.info('Extracting data')\n",
    "    df = extract_data('json_responses', [search_term])\n",
    "\n",
    "    shutil.rmtree('json_responses')\n",
    "\n",
    "    # Save the extracted data to a CSV file\n",
    "    logging.info(f\"Saving extracted data to 'output.csv'\")\n",
    "    output_folder='output'\n",
    "    output_filename='output.csv'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    df.to_csv(f'{output_folder}/{output_filename}', index=False)\n",
    "    logging.info('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
